# üõí Project Goal

This project aims to **predict the price of grocery items** based on  
their **product description and image**, as part of the **Amazon ML Challenge 2025**. The challenge involves analyzing complex relationships between product attributes (brand, specifications, quantity) and their corresponding market price.

# üß† Approach Overview: *The "Committee of Experts" Ensemble*

Initial explorations revealed the limitations of using a single model or simple feature combinations for this **multimodal problem** (text + image). A key insight was that the dataset consisted primarily of grocery items, where price drivers vary significantly across categories (e.g., coffee ‚òï vs. sauces ü•´).

Therefore, a more sophisticated **ensemble strategy**, termed the **"Committee of Experts"**, was adopted. This approach leverages specialized models for each data modality:

- **üìÑ The "Text Expert"**: A **Gradient Boosting model (LightGBM)** trained on carefully engineered features derived from the product description and category information.
- **üñºÔ∏è The "Image Expert"**: A **fine-tuned Convolutional Neural Network (EfficientNet-B0)** specifically trained to predict `log_price` directly from product images, capturing visual cues related to value.

The final price prediction is generated by **blending the outputs** of these two expert models using a weighted average, combining their distinct strengths.

# üìä Data

The dataset provided contains:

::: center
  **Column Name**     **Description**
  ------------------- -------------------------------------------------------------------------
  `sample_id`         Unique identifier
  `catalog_content`   Text description including title, details, and Item Pack Quantity (IPQ)
  `image_link`        URL to the product image
  `price`             Target variable (available only in the training set)
:::

# üõ†Ô∏è Methodology

## üîπ Data Preparation and Categorization

- **Log Transformation**  
  The target variable `price` exhibited a strong right skew. A `log1p` transformation (`log_price = np.log1p(price)`) was applied to stabilize the target distribution. Models were trained to predict `log_price`.

- **Intelligent Categorization**  
  Recognizing the grocery-specific nature, an initial **rule-based system** followed by a **data-driven keyword scoring engine** was developed (using **TF-IDF** on the "Other" category) to classify products into meaningful categories such as:

  - ‚òï *Coffee & Tea*
  - üç´ *Chocolates & Candy*
  - ü•´ *Sauces*

  This approach significantly improved data understanding compared to simple rule-based methods.

## üìù The "Text Expert" (LightGBM)

This model focuses on extracting signals from the `catalog_content`.

### Feature Engineering

- **Item Pack Quantity (IPQ):** Extracted using regex ‚Äî quantity is a primary price driver.
- **Brand:** Extracted using NLP (`spaCy` for NER) during EDA and refined with regex for the final pipeline. Rare brands were grouped under `"Other"` to reduce noise.
- **Category:** The intelligently derived category was used as a crucial categorical feature.
- **Text Representation:** TF-IDF (Term Frequency‚ÄìInverse Document Frequency) with n-grams (1,2) converted raw text into a high-dimensional sparse matrix capturing important keywords and phrases.

### Modeling

A **LightGBM regressor** was trained on the combination of engineered numerical/categorical features and TF-IDF features.

## üñºÔ∏è The "Image Expert" (Fine-Tuned EfficientNet)

This model captures visual price signals that text-based models might miss.

### Rationale

Visual cues like **packaging quality, branding, product appearance**, and **bulk presentation** influence perceived value.

### Fine-Tuning

- Used **EfficientNet-B0** pre-trained on **ImageNet**.
- The final classification layer was replaced with a **single regression neuron** to output `log_price`.
- Model trained directly on product images with `log_price` as the target.

### Checkpointing

The best model (based on validation loss) was saved after each epoch to prevent overfitting. Final saved model: `best_vision_model.pth`.

## ü§ù Ensembling Strategy

Predictions from both expert models were combined for the final estimate.

### Steps

1. **Prediction Generation:** Both the **Text Expert (LightGBM)** and **Image Expert (EfficientNet)** predict `log_price` for the test set.
2. **Inverse Transformation:** Convert predictions back to original scale using:

    ```python
    price = np.expm1(log_price)
    ```

3. **Weighted Blending**  

   The final price was calculated as a **weighted average** of the two predictions:

   $$
   \text{Final Price} = (\text{Text\_Price} \times W_{\text{text}}) + (\text{Image\_Price} \times W_{\text{image}})
   $$

   Experimentation on the validation set showed that a **higher weight for the text model** yielded the best results, with the optimal blend being approximately **90% Text Expert / 10% Image Expert**.

# üìà Results

The final ensemble model, combining the strengths of detailed text analysis and specialized visual understanding, achieved a **SMAPE score of approximately 53.02%** on the validation set.

This score reflects the performance of the blended **"Committee of Experts"** approach.

> **‚ö†Ô∏è Self-correction:** While the feature-engineered model alone achieved ~51%, the ensemble score with the DL component was ~53%. This README accurately reflects the score achieved by the described ensemble method.

# üíª Technology Stack

- **Python 3**
- `pandas`, `numpy`
- `scikit-learn` ‚Äî for preprocessing, TF-IDF, and model evaluation
- `LightGBM` ‚Äî for the Text Expert model
- `PyTorch` ‚Äî for the Image Expert model (training and inference)
- `timm` ‚Äî PyTorch Image Models library for EfficientNet
- `transformers` ‚Äî Hugging Face library, used for initial zero-shot experiments
- `Pillow` ‚Äî for image loading
- `joblib` ‚Äî for saving/loading models
- `tqdm` ‚Äî for progress bars

# üèÉ Running the Code

The primary code demonstrating the final ensemble approach is in `final_dl_training_notebook.ipynb`. To generate the submission file `test_result2.csv`, follow these steps:

1. **Install dependencies**  
   Ensure all dependencies from `requirements.txt` are installed:

    ```bash
    pip install -r req.txt
    ```

2. **Place model file**  
   Place the trained image model `best_vision_model.pth` in the project's root directory.

3. **Place data files**  
   Place the necessary data files (`train_processed.feather`, `test.csv`) in a `dataset/` subfolder.

4. **Ensure image folder**  
   Ensure the `images/` folder containing all product images is present.

5. **Run the notebook**  
   Run the cells in `final_dl_training_notebook.ipynb` sequentially. The final cell will generate `test_result2.csv`.

